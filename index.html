<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="icon" type="image/png" href="assets/icon.jpg" />
  <title>Persian Culinary RAG</title>
  <meta property="og:title" content="Persian Culinary RAG: Multimodal Retrieval and Generation for Text–Image Food Queries">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://vlm-cross-modal-reps.github.io">
  <!-- <meta name="description" content="Task representations in VLMs are consistent across modality (text, image) and specification (example, instruction)."> -->
  <!-- <meta name="keywords" content="vision-language models, multimodal models, task vectors, function vectors, in-context learning, interpretability"> -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RQ2WVP2RSN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-RQ2WVP2RSN');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">
  <link rel="stylesheet" href="./static/css/fonts.css">
  <script src="./static/js/interactions.js"></script>
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding: 3rem 1.5rem 0.5rem 1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Persian Culinary RAG: Multimodal Retrieval and Generation for Text–Image Food Queries</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://linkedin.com/in/mohammad-hossein-eslami-152586330" target="_blank">Mohammad.H Eslami</a>
            </span>
            <span class="author-block">
              <a href="https://github.com/arshiaizd" target="_blank">Arshia Izadyari</a>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sadegh-mohammadian-810468281/" target="_blank">Sadegh Mohammadian</a>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/fatemeh-askari-4045b9235" target="_blank">Fateme Askari</a>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/ali-rahimiakbar-1a5762221?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app" target="_blank">Ali RahimiAkbar</a>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/mmvahedi/" target="_blank">Mohammad.M Vahedi</a>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sharif University of Technology</span>
          </div>
          <div class="is-size-5 publication-authors">
            <br>
            <!-- <span class="author-block"><b>ICML 2026</b></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/NLP-Final-Projects/Food_rag_3" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
              <a href="https://huggingface.co/spaces/parsi-ai-nlpclass/Persian-Food-RAG" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height:1.2em; vertical-align: middle;">
                </span>
                <span>dataset</span>
              </a>
            </span>
              <span class="link-block">
              <a href="https://huggingface.co/spaces/parsi-ai-nlpclass/Persian-Food-RAG" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Faceeeee" style="height:1.2em; vertical-align: middle;">
                </span>
                <span>Web demo</span>
                
              </a>
              </div>
          <br>
          <br>
          <img src="assets/abs.jpg" 
          alt="Graphical abstract of Grounding IDs" 
          class="max-width-results" />

          <!-- <div class="finding-box">
            Text here!!</div> -->
        </div>
            </span>            
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="padding-top: 1rem;">
  <div class="container is-max-desktop">
    <div class="is-centered has-text-centered">
      <p>
        <!-- <b>TLDR:</b> #TODO<br> -->
      </p>
      <br>
      <!-- Image or video here -->
      <!-- <div style="max-width: 720px;" class="max-width-content">
        <video id="teaser" muted playsinline onclick="playVideo('teaser')">
          <source src="assets/teaser.m4v" type="video/mp4">
        </video>
        <br>
        <button class="button is-white btn-teaser" onclick="playVideo('teaser')">
          <img style="margin-right: 5px;" src="assets/hand.svg" />
          Click to animate figure
        </button> -->
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-left">
          <p>
          We present a Persian, food-domain retrieval-augmented generation (RAG) system that combines a dual-modality retriever with a lightweight generator. Building on our prior corpus and an added Kaggle recipe collection (1,737 entries; 1,393 unique dishes), we expand the index with web-sourced photos of dishes \emph{and} systematically collected images of key ingredients to strengthen image-grounded queries. The retriever pairs a Persian text encoder (Glot-500) with a fine-tuned CLIP vision--text encoder (vision-fa-clip/SajjadAyoubi) trained with a multi-positive contrastive objective to handle multiple instructions per dish. Cross-modal embeddings enable answering both text-only and image+text questions by retrieving pertinent evidence and conditioning the generator. On held-out multiple-choice sets, the RAG setup improves performance for ingredient-triggered and image-grounded queries with a lighter generator, while gains are mixed for a stronger generator.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Introduction</h2>
        <div class="max-width-content">
        


          <br>
          <div class="content has-text-left">
            <p>
              Retrieval links user queries to relevant evidence, enabling grounded and controllable answers in knowledge-intensive settings. In the culinary domain, retrieval is especially helpful for (i) surfacing recipes and procedural steps for a named dish, (ii) answering ingredient- or diet-centric questions (e.g., substitutions, halal/vegan constraints), (iii) disambiguating regional variants and homonymous dishes, and (iv) bridging images and text when a photo of plated food, garnish, or a raw ingredient is the primary cue. Pairing retrieval with generation further reduces hallucinations by conditioning responses on retrieved context, including cross-modal evidence.
            </p>
          </div>

          <!-- <img src="assets/graphical_abstract.png" 
          alt="Graphical abstract of Grounding IDs" 
          class="max-width-results" /> -->

          <!-- <div class="finding-box">
            Text here!!</div> -->
        </div>
      </div>
    </div>
  </div>
</section>



<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Method Summary</h2>
        <div class="max-width-content">

          
          <br>
          <div class="content has-text-left">
            <p>
            Our system is built on a Retrieval-Augmented Generation (RAG) architecture tailored for the Persian culinary domain. At its core is a shared representation learning module where both images and text passages are mapped into a unified vector space using a CLIP-style vision tower and a fine-tuned Glot-500 text encoder, respectively. For any given query, whether text or image-based, we leverage these embeddings to perform an efficient similarity search against a FAISS index of our entire culinary text corpus. The top-ranked documents are then pooled to form an evidence block, which is injected directly into the prompt for the generative model to produce a contextually grounded answer. A key innovation in our methodology is an ingredient-aware training strategy; by treating images of a dish's main ingredients as additional positive examples during fine-tuning, we encourage the model to develop a more holistic concept of each dish, thereby enhancing the retriever's robustness, especially for ingredient-based queries.
            </p>
          </div>

          <!-- <img src="assets/attention_main_fig.png" 
          alt="" 
          class="max-width-results" />

          <img src="assets/alignment_main_fig.png" 
          alt="" 
          class="max-width-results" /> -->

        </div>
      </div>
    </div>
  </div>
</section>


<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Dataset Properties</h2>
        <div class="max-width-content">

          <br>
          <div class="content has-text-left">
            <p>
              Our project is built upon a novel multimodal dataset of Iranian foods, which we created from scratch. This dataset contains 1,737 total entries covering 1,393 unique dishes, each featuring descriptive passages and corresponding question-answer pairs generated by an LLM. A key property of this corpus is its multimodal nature; every entry is enriched with representative images of both the final plated dish and its three main ingredients, enabling a more holistic visual understanding. The dataset was constructed through web scraping and LLM-powered cleaning, and we deliberately avoided aggressive text preprocessing to preserve its full linguistic richness for our RAG system.
            </p>
          </div>

          <!-- <img src="assets/activation_swap.png" 
          alt="" 
          class="max-width-results" />

          <img src="assets/cma_layer_head.png" 
          alt="" 
          class="max-width-results" /> -->

        </div>
      </div>
    </div>
  </div>
</section>


<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Evaluation and Results</h2>
        <div class="max-width-content">

          <br>
          <div class="content has-text-left">
            <p>
            We evaluated our system on a custom set of 50 multiple-choice questions (30 text-only, 20 image-based). Our key finding was the differential impact of RAG on generators of varying strengths.
            </p>
          </div>

          <img src="assets/results.png" 
          alt="" 
          class="max-width-results" />

          <img src="assets/result2.png" 
          alt="" 
          class="max-width-results" />

        </div>
      </div>
    </div>
  </div>
</section>


<!-- for more section :
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Title here!!</h2>
        <div class="max-width-content"> -->
          <!-- Video or image here -->
          <!-- <video class="max-width-results" id="instruction" muted playsinline onclick="playVideo('instruction')">
            <source src="assets/instruction.m4v" type="video/mp4">
          </video> -->
          
          <!-- <br>
          <div class="content has-text-left">
            <p>
            Text here
            </p>
          </div>
          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<hr>
<section class="section">

  <div class="container is-max-desktop content">
    <!-- <h2 class="title is-4">Relevant Readings</h2>
    <p>
      If you like this work, these other projects might also interest you.
    </p>
    <ul>
      <li>
        <p>
          Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
          <a href="https://arxiv.org/abs/2005.11401" target="_blank">Hendel et al., 2023</a>; 
          <a href="https://functions.baulab.info" target="_blank">Todd et al., 2024</a>;
          <a href="https://arxiv.org/abs/2404.05729" target="_blank">Hojel et al., 2024</a>
        </p>
      </li>
    </ul> -->



    <h2 class="title is-4">BibTeX</h2>
    <pre><code>
    @misc{NlpAll2025PersianFoodRag,
      title={Persian Culinary RAG: Multimodal Retrieval and Generation for Text-Image Food Queries}, 
      author={Mohammad Hossein Eslami and Arshia Izadyari and Sadegh Mohammadian and Fateme Askari and Ali Rahimi Akbar and Mohammad Mahdi Vahedi},
      year={2025},
      eprint={},
      archivePrefix={},
      url={}, 
    }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template is based on the
            <a href="https://nerfies.github.io">Nerfies</a> and
            <a href="https://readout-guidance.github.io">Readout Guidance</a> project pages.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
